---
id: calendar
name: Calendar
heading: Calendar
subheading: Calendar&#58;
image: ""
---

This a draft schedule and is subject to change.  

|Schedule           | Broad Area                | Reading List
|-----------|------------------------|---------
|**Week Jan 7** | Course Overview & Intro to RL	|Slides:<br /> [Lecture 1](assets/slides/lec1.pdf),<br />[Presentation Template Slides](assets/slides/template.pptx)<br /><br />Papers:<br />[Human Learning in Atari](https://core.ac.uk/download/pdf/141473125.pdf)|
|**Week Jan 14** | 	Imitation Learning: supervised	|Slides:<br /> [Lecture 2](assets/slides/lec2.pdf), <br />[End To End Learning Slides](assets/slides/lec2_endtoend.pdf),<br />[Behavioural Cloning Slides](assets/slides/lec2_behaviorcloning.pdf)<br /><br /> Overview:<br />[An Invitation To Imitation](https://www.ri.cmu.edu/publications/an-invitation-to-imitation/),<br />[Dagger: A reduction of imitation learning and structured prediction to no-regret online learning](https://arxiv.org/pdf/1011.0686.pdf),<br /> [End to End Learning for Self-Driving Cars](https://arxiv.org/abs/1604.07316),<br /> [Behavioral Cloning from Observation](https://www.ijcai.org/proceedings/2018/0687.pdf)<br /><br />Optional: <br />[ALVINN: An autonomous land vehicle in a neural network](https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf),<br />[ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst](https://arxiv.org/abs/1812.03079),<br />[Apprenticeship Learning via Inverse Reinforcement Learning](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)|
|**Week Jan 21** | 	Policy Gradients	|Slides:<br />[Lecture 3](assets/slides/lec3.pdf),<br />[Policy Gradient Methods](assets/slides/lec3_pgm.pdf)<br /><br />Papers:<br />[Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf),<br />[Trust region policy optimization: deep RL with natural policy gradient and adaptive step size](https://arxiv.org/pdf/1502.05477) (TRPO),<br /> [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971) (DDPG), <br /> [Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines](https://arxiv.org/pdf/1803.07246.pdf)<br /><br />Optional:<br />SB Ch: 13, <br /> [Reinforcement learning of motor skills with policy gradients](https://www.sciencedirect.com/science/article/pii/S0893608008000701)|
|**Week Jan 28** | 	Actor-Critic Methods+ Value Based methods	|Slides:<br />[Asynchronous Methods for Deep Reinforcement Learning](assets/slides/lec4_actorcritic.pdf),<br />[Soft Actor-Critic](assets/slides/lec4_sac.pdf)<br /><br />Papers:<br />[Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783),<br />[Soft Actor-Critic Algorithms and Applications](https://arxiv.org/abs/1812.05905),<br />[IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https://arxiv.org/abs/1802.01561),<br />[High-confidence error estimates for learned value functions](https://arxiv.org/abs/1808.09127)<br /><br />Optional:<br />SB Ch: 13,<br />[Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)
|
|**Week Jan 28** | 	(before class)	| **<span style="color:#b32425">Project Proposal Due</span>**|
|**Week Feb 4** | Q-Value based RL|Slides:<br />[Double DQN and Dueling DQN](assets/slides/lec5_ddqn.pdf),<br />[QT-OPT and q2-opt](assets/slides/lec5_qtopt.pdf),<br />[PCL and TrustPCL](assets/slides/lec5_pcl.pdf),<br />[Rainbow and Impala](assets/slides/lec5_rainbow.pdf)<br /><br />Papers:<br/>[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) (DQN) (will be covered by Animesh),<br /><br /> [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (Double DQN) + [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) (Dueling DQN)<br /> Dueling DQN + Double DQN (1 student)<br /><br /> [QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation](https://arxiv.org/abs/1806.10293) (Qt-Opt, also discuss the update in [q2-opt](https://arxiv.org/abs/1910.02787)) (1 student)<br /><br /> [Bridging the Gap Between Value and Policy Based Reinforcement Learning](https://arxiv.org/abs/1702.08892) (PCL, also discuss the update in [TrustPCL](https://arxiv.org/pdf/1707.01891.pdf)) (2 students)<br /><br /> [Rainbow - Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298) (also discuss [Impala](https://arxiv.org/abs/1802.01561)) (1 student)<br /><br /> Optional:<br /> [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) (TD3)<br /> [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)|
|**Week Feb 11** | 	Distributional RL	|[A Comparative Analysis of Expected and Distributional Reinforcement Learning](https://arxiv.org/abs/1901.11084) (2 students),<br /><br /> [Implicit Quantile Networks for Distributional Reinforcement Learning](https://arxiv.org/abs/1806.06923) (1 student),<br /><br /> [Statistics and Samples in Distributional Reinforcement Learning](https://arxiv.org/abs/1902.08102) (1 student),<br /><br /> [Value Function in Frequency Domain and the Characteristic Value Iteration Algorithm](https://papers.nips.cc/paper/9620-value-function-in-frequency-domain-and-the-characteristic-value-iteration-algorithm) (1 student),<br /><br /> [An analysis of categorical distributional reinforcement learning](https://arxiv.org/abs/1802.08163) (1 student)<br /><br /> Optional:<br /> [Reinforcement learning with Gaussian processes](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6420&rep=rep1&type=pdf),<br /> [Nonparametric return distribution approximation for reinforcement learning](https://pdfs.semanticscholar.org/1ec2/6e05c2577154213e1668ddd374e4da663309.pdf),<br /> [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/pdf/1707.06887.pdf) (C51),<br /> [Distributed Distributional Deterministic Policy Gradients](https://arxiv.org/abs/1804.08617) (D4PG)|
|**Week Feb 18** | 	Model-Based RL	|[PILCO: Probabilistic Inference for Learning COntrol](http://mlg.eng.cam.ac.uk/carl/pilco/) (1 student),<br /><br /> [Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees](https://arxiv.org/abs/1807.03858) (SLBO) (1 student),<br /><br /> [Model-Based Reinforcement Learning via Meta-Policy Optimization](https://arxiv.org/abs/1809.05214) (MB-MPO) (1 student),<br /><br /> [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603) (1 student),<br /><br /> [Iterative Value-Aware Model Learning](https://papers.nips.cc/paper/8121-iterative-value-aware-model-learning.pdf) (refer to [supplement](https://papers.nips.cc/paper/8121-iterative-value-aware-model-learning-supplemental.zip) as well) (2 students),<br /><br /> [Intro to ILQR](https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf) (Animesh will cover)<br /><br /> Optional:<br /> [Benchmarking Model-Based Reinforcement Learning](https://arxiv.org/abs/1907.02057),<br /> PlaNet [Learning Latent Dynamics for Planning from Pixels](https://arxiv.org/abs/1811.04551),<br /> [Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images](https://arxiv.org/abs/1506.07365)(E2C),<br /> [Robust locally-linear controllable embedding](https://arxiv.org/abs/1710.05373)(RCE),<br /> [World Models](https://worldmodels.github.io/),<br /> [Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models](https://arxiv.org/abs/1805.12114) (PETS)|
|**Week Feb 25** | 	(before class)	|**<span style="color:#b32425">Mid-Term Project Report Due</span>**|
|**Week Feb 25** | 	Imitation: Inverse RL	|[Generative Adversarial Imitation Learning](https://arxiv.org/abs/1606.03476),<br />[Maximum Entropy Inverse Reinforcement Learning](https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf),<br />[Provably Efficient Imitation Learning from Observation Alone](https://arxiv.org/abs/1905.10948)<br /><br />Optional:<br />[Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation](https://arxiv.org/abs/1707.03374)|
|**Week Feb 28** | 	Friday 9am	| **<span style="color:#b32425">Take Home Midterm (24 hours to turn-in)</span>**|
|**Week Mar 3** | Exploration in RL	|[Deep Exploration via Bootstrapped DQN](https://arxiv.org/abs/1602.04621),<br />[Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models](https://arxiv.org/abs/1507.00814),<br /> [Large-Scale Study of Curiosity-Driven Learning Episodic Curiosity through Reachability](https://arxiv.org/abs/1808.04355),<br />[Approximate Exploration through State Abstraction](https://arxiv.org/abs/1808.09819),<br /> [Go-Explore: a New Approach for Hard-Exploration Problems](https://arxiv.org/abs/1901.10995)<br /><br /> Optional:<br /> [Parameter Space Noise for Exploration](https://arxiv.org/abs/1706.01905),<br /> [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495),<br /> [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894)|
|**Week Mar 10** | 	Bayesian RL	|[Bayesian Reinforcement Learning: A Survey](https://arxiv.org/abs/1609.04436),<br />[VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning](https://arxiv.org/abs/1910.08348)|
|**Week Mar 17** | 	Hierarchical RL	|[Building Portable Options: Skill Transfer in Reinforcement Learning](https://www.ijcai.org/Proceedings/07/Papers/144.pdf),<br /> [Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning](https://www.sciencedirect.com/science/article/pii/S0004370299000521),<br /> Variational Option Discovery Algorithms|
|**Week Mar 24** | 	Project Presentation	||
|**Week Mar 31** | 	Project Presentation	||
|**Week Apr 7** | [Buffer]	||
|**Week Apr 7** | Tues 11:59 pm	|**<span style="color:#b32425">Final Project Report Due</span>**|
